# -*- coding: utf-8 -*-
"""kidney_disease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MaNB-fn0mNFjcJlc6udbZa7fK-zxTMB7
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/kidney_disease_dataset.csv')
print(data.head())
print(data.info())

"""**Reasoning**:
Display the descriptive statistics and the info of the dataframe to understand the data distribution and types.


"""

display(data.describe())
display(data.info())

"""**Reasoning**:
Periksa apakah ada nilai yang hilang dalam dataset dengan menghitung jumlah nilai yang hilang per kolom.


"""

missing_values = data.isnull().sum()
print("Jumlah nilai yang hilang per kolom:")
print(missing_values)

"""**Reasoning**:
Create visualizations as requested: histograms for numerical columns, a scatter plot for 'Creatinine_Level' vs 'BUN', and a box plot for 'GFR' by 'CKD_Status' to understand variable distributions and relationships.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Histograms for numerical columns
numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    plt.subplot(3, 3, i + 1)
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Step 2: Scatter plot for 'Creatinine_Level' and 'BUN'
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Creatinine_Level', y='BUN', data=data)
plt.title('Creatinine Level vs BUN')
plt.xlabel('Creatinine Level')
plt.ylabel('BUN')
plt.show()

# Step 3: Box plot for 'GFR' by 'CKD_Status'
plt.figure(figsize=(8, 6))
sns.boxplot(x='CKD_Status', y='GFR', data=data)
plt.title('GFR Distribution by CKD Status')
plt.xlabel('CKD Status (0: Non-CKD, 1: CKD)')
plt.ylabel('GFR')
plt.xticks([0, 1], ['Non-CKD', 'CKD'])
plt.show()

"""**Reasoning**:
Calculate the correlation matrix for the numerical columns and then visualize it as a heatmap with annotations to show the correlation values.


"""

# Calculate the correlation matrix
correlation_matrix = data.corr()

# Create a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Matriks Korelasi Variabel Numerik')
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   The dataset contains 2304 entries with no missing values across all columns.
*   Numerical columns include `Age`, `Creatinine_Level`, `BUN`, `GFR`, and `Urine_Output`.
*   Histograms show the distribution of numerical variables.
*   There appears to be a positive correlation between `Creatinine_Level` and `BUN`.
*   The box plot indicates that the GFR distribution is lower for individuals with CKD (`CKD_Status` = 1) compared to those without CKD (`CKD_Status` = 0).
*   The correlation analysis revealed strong positive correlations between `Creatinine_Level` and `BUN`, and strong negative correlations between `GFR` and `Creatinine_Level` as well as between `GFR` and `BUN`.

### Insights or Next Steps

*   The inverse relationship between GFR and both Creatinine Level and BUN is expected, as these are indicators of kidney function. Lower GFR suggests reduced kidney function, which typically leads to higher levels of creatinine and BUN in the blood.
*   Further analysis could involve building a predictive model for CKD status using the identified key features like GFR, Creatinine Level, and BUN.

**Reasoning**:
Identify numerical columns for scaling, import and apply StandardScaler to these columns, and create a new DataFrame with the scaled features.
"""

from sklearn.preprocessing import StandardScaler

# Identify numerical columns that would benefit from scaling
# Exclude binary columns that are already 0 or 1
numerical_cols_to_scale = ['Age', 'Creatinine_Level', 'BUN', 'GFR', 'Urine_Output']

# Instantiate the scaler
scaler = StandardScaler()

# Apply the scaler to the identified numerical columns
data_scaled = data.copy()
data_scaled[numerical_cols_to_scale] = scaler.fit_transform(data[numerical_cols_to_scale])

# Display the first few rows of the scaled data
display(data_scaled.head())

"""**Reasoning**:
Identify and handle outliers in the scaled numerical columns using the IQR method and capping.


"""

# Step 1 & 2: Identify outliers using IQR on scaled numerical columns
numerical_cols_to_scale = ['Age', 'Creatinine_Level', 'BUN', 'GFR', 'Urine_Output']

Q1 = data_scaled[numerical_cols_to_scale].quantile(0.25)
Q3 = data_scaled[numerical_cols_to_scale].quantile(0.75)
IQR = Q3 - Q1

# Define outlier bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Step 3: Handle outliers by capping
# Cap values outside the outlier bounds
for col in numerical_cols_to_scale:
    data_scaled[col] = data_scaled[col].clip(lower=lower_bound[col], upper=upper_bound[col])

# Display descriptive statistics after capping to see the effect
display(data_scaled[numerical_cols_to_scale].describe())

# Step 4: Briefly explain the methods and reasoning
print("Outlier detection method: Interquartile Range (IQR)")
print("Reasoning: IQR is a robust method for detecting outliers, less sensitive to extreme values than z-scores. Outliers are identified as values falling below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.")
print("\nOutlier handling method: Capping")
print("Reasoning: Capping replaces outlier values with the nearest boundary value (lower or upper bound). This approach reduces the influence of outliers without removing data points, preserving the sample size and general distribution shape within the defined bounds.")

"""**Reasoning**:
Import train_test_split, define features and target, and split the scaled data into training and testing sets using stratification.


"""

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = data_scaled.drop(['CKD_Status', 'Dialysis_Needed'], axis=1)
y = data_scaled['CKD_Status']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Print the shapes of the resulting sets to verify
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""## Summary:

### Data Analysis Key Findings

*   All variables in the dataset were initially in numerical formats (`int64` or `float64`). Binary conceptual categorical variables (`Diabetes`, `Hypertension`, `CKD_Status`, `Dialysis_Needed`) were already represented numerically as 0s and 1s, thus requiring no additional encoding.
*   Numerical columns 'Age', 'Creatinine\_Level', 'BUN', 'GFR', and 'Urine\_Output' were scaled using `StandardScaler`.
*   Outliers in the scaled numerical columns were detected using the Interquartile Range (IQR) method and handled by capping the values within the calculated bounds (Q1 - 1.5\*IQR and Q3 + 1.5\*IQR).
*   The dataset was successfully split into training (80%) and testing (20%) sets using `train_test_split`, with 'CKD\_Status' as the target variable and stratification applied to maintain class distribution. The resulting sets had shapes of (1843, 7) for training features, (461, 7) for testing features, (1843,) for training target, and (461,) for testing target.

### Insights or Next Steps

*   The data is now preprocessed and ready for training a machine learning model to predict 'CKD\_Status'.
*   Given the binary nature of the target variable 'CKD\_Status', classification models would be appropriate for the next steps.

**Reasoning**:
Import a suitable classification model, Logistic Regression, as it is a simple yet effective model for binary classification.
"""

from sklearn.linear_model import LogisticRegression

# Instantiate the Logistic Regression model
model = LogisticRegression(random_state=42)

"""**Reasoning**:
Import evaluation metrics, train the model, make predictions, and calculate evaluation metrics.


"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Fit the Logistic Regression model to the training data
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print the evaluation metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

"""**Reasoning**:
Import the necessary libraries for hyperparameter tuning, define the parameter grid, instantiate GridSearchCV, and fit it to the training data. Then print the best parameters found.


"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define the parameter grid
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2']
}

# Instantiate GridSearchCV
grid_search = GridSearchCV(LogisticRegression(solver='liblinear', random_state=42), param_grid, cv=5, scoring='f1')

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters found:")
print(grid_search.best_params_)

"""**Reasoning**:
Instantiate a new Logistic Regression model with the best hyperparameters, train it on the training data, make predictions on the test data, and evaluate the performance using accuracy, precision, recall, and F1-score.


"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Instantiate a new Logistic Regression model with the best hyperparameters
best_model = LogisticRegression(C=grid_search.best_params_['C'],
                                penalty=grid_search.best_params_['penalty'],
                                solver='liblinear', # Use 'liblinear' solver for 'l1' penalty
                                random_state=42)

# Train the model on the entire training dataset
best_model.fit(X_train, y_train)

# Make predictions on the testing dataset
y_pred_tuned = best_model.predict(X_test)

# Calculate evaluation metrics for the tuned model
accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
precision_tuned = precision_score(y_test, y_pred_tuned)
recall_tuned = recall_score(y_test, y_pred_tuned)
f1_tuned = f1_score(y_test, y_pred_tuned)

# Print the evaluation metrics
print("Metrics for the initial model:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print("\nMetrics for the tuned model:")
print(f"Accuracy: {accuracy_tuned:.4f}")
print(f"Precision: {precision_tuned:.4f}")
print(f"Recall: {recall_tuned:.4f}")
print(f"F1-score: {f1_tuned:.4f}")

"""## Summary:

### Data Analysis Key Findings

*   An initial Logistic Regression model was trained and evaluated, achieving an accuracy of 0.8330, precision of 0.8435, recall of 0.8255, and an F1-score of 0.8344 on the test set.
*   Hyperparameter tuning using `GridSearchCV` identified the best hyperparameters for Logistic Regression as `{'C': 0.01, 'penalty': 'l1'}`.
*   Training and evaluating a Logistic Regression model with the best hyperparameters resulted in improved performance on the test set: accuracy of 0.8525, precision of 0.8615, recall of 0.8468, and an F1-score of 0.8541.

### Insights or Next Steps

*   Hyperparameter tuning slightly improved the Logistic Regression model's performance for predicting CKD status.
*   Further exploration with different classification algorithms or feature engineering could potentially lead to better model performance.

**Reasoning**:
Import a suitable classification model, Logistic Regression, as it is a simple yet effective model for binary classification.
"""

from sklearn.linear_model import LogisticRegression

# Instantiate the Logistic Regression model
model = LogisticRegression(random_state=42)

"""**Reasoning**:
Import evaluation metrics, train the model, make predictions, and calculate evaluation metrics.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Fit the Logistic Regression model to the training data
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print the evaluation metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

"""**Reasoning**:
Import the necessary libraries for hyperparameter tuning, define the parameter grid, instantiate GridSearchCV, and fit it to the training data. Then print the best parameters found.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define the parameter grid
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2']
}

# Instantiate GridSearchCV
grid_search = GridSearchCV(LogisticRegression(solver='liblinear', random_state=42), param_grid, cv=5, scoring='f1')

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters found:")
print(grid_search.best_params_)

"""**Reasoning**:
Instantiate a new Logistic Regression model with the best hyperparameters, train it on the training data, make predictions on the test data, and evaluate the performance using accuracy, precision, recall, and F1-score.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Instantiate a new Logistic Regression model with the best hyperparameters
best_model = LogisticRegression(C=grid_search.best_params_['C'],
                                penalty=grid_search.best_params_['penalty'],
                                solver='liblinear', # Use 'liblinear' solver for 'l1' penalty
                                random_state=42)

# Train the model on the entire training dataset
best_model.fit(X_train, y_train)

# Make predictions on the testing dataset
y_pred_tuned = best_model.predict(X_test)

# Calculate evaluation metrics for the tuned model
accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
precision_tuned = precision_score(y_test, y_pred_tuned)
recall_tuned = recall_score(y_test, y_pred_tuned)
f1_tuned = f1_score(y_test, y_pred_tuned)

# Print the evaluation metrics
print("Metrics for the initial model:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print("\nMetrics for the tuned model:")
print(f"Accuracy: {accuracy_tuned:.4f}")
print(f"Precision: {precision_tuned:.4f}")
print(f"Recall: {recall_tuned:.4f}")
print(f"F1-score: {f1_tuned:.4f}")

"""## Summary:

### Data Analysis Key Findings

* An initial Logistic Regression model was trained and evaluated, achieving an accuracy of {{accuracy:.4f}}, precision of {{precision:.4f}}, recall of {{recall:.4f}}, and an F1-score of {{f1:.4f}} on the test set.
* Hyperparameter tuning using `GridSearchCV` identified the best hyperparameters for Logistic Regression as `{{grid_search.best_params_}}`.
* Training and evaluating a Logistic Regression model with the best hyperparameters resulted in improved performance on the test set: accuracy of {{accuracy_tuned:.4f}}, precision of {{precision_tuned:.4f}}, recall of {{recall_tuned:.4f}}, and an F1-score of {{f1_tuned:.4f}}.

### Insights or Next Steps

* Hyperparameter tuning slightly improved the Logistic Regression model's performance for predicting CKD status.
* Further exploration with different classification algorithms or feature engineering could potentially lead to better model performance.

# Task
Train and evaluate multiple classification models to predict tumor malignancy, including hyperparameter tuning for the best performing model.

**Reasoning**:
List at least three different classification algorithms suitable for a binary classification problem and provide a brief justification for selecting each model.
"""

print("Suitable Classification Models for Binary Classification:")

print("\n1. Logistic Regression:")
print("   - Justification: A simple yet powerful linear model that estimates the probability of a binary outcome. It's computationally efficient and provides interpretable coefficients, making it a good baseline model.")

print("\n2. Support Vector Machine (SVM):")
print("   - Justification: Effective in high-dimensional spaces and when there is a clear margin of separation between classes. SVMs can use different kernel functions to handle non-linear relationships.")

print("\n3. Random Forest:")
print("   - Justification: An ensemble method that builds multiple decision trees and merges their predictions. It's robust to overfitting, can handle non-linear data, and provides feature importance scores.")

print("\n4. Gradient Boosting (e.g., LightGBM, XGBoost, CatBoost):")
print("   - Justification: Another powerful ensemble technique that builds trees sequentially, with each new tree correcting errors of the previous ones. Often achieves high accuracy and can capture complex patterns.")

print("\n5. K-Nearest Neighbors (KNN):")
print("   - Justification: A non-parametric lazy learning algorithm that classifies data points based on the majority class of their nearest neighbors. Simple to understand and implement, and effective for datasets with well-separated classes.")

"""**Reasoning**:
Import the selected classification models and train each one on the training data.


"""

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

# Instantiate the selected classification models
# Logistic Regression is already imported and instantiated as 'model' in a previous cell
svm_model = SVC(random_state=42)
rf_model = RandomForestClassifier(random_state=42)
gb_model = GradientBoostingClassifier(random_state=42)
knn_model = KNeighborsClassifier() # KNN does not have a random_state

# Train each model on the training data
print("Training Logistic Regression model...")
model.fit(X_train, y_train)
print("Logistic Regression model trained.")

print("Training Support Vector Machine model...")
svm_model.fit(X_train, y_train)
print("Support Vector Machine model trained.")

print("Training Random Forest model...")
rf_model.fit(X_train, y_train)
print("Random Forest model trained.")

print("Training Gradient Boosting model...")
gb_model.fit(X_train, y_train)
print("Gradient Boosting model trained.")

print("Training K-Nearest Neighbors model...")
knn_model.fit(X_train, y_train)
print("K-Nearest Neighbors model trained.")

"""**Reasoning**:
Evaluate the performance of each trained model on the testing data using accuracy, precision, recall, and F1-score and print the results.


"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Evaluate Logistic Regression
y_pred_lr = model.predict(X_test)
accuracy_lr = accuracy_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)
print("Logistic Regression Metrics:")
print(f"Accuracy: {accuracy_lr:.4f}")
print(f"Precision: {precision_lr:.4f}")
print(f"Recall: {recall_lr:.4f}")
print(f"F1-score: {f1_lr:.4f}")
print("-" * 30)

# Evaluate Support Vector Machine
y_pred_svm = svm_model.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm)
recall_svm = recall_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm)
print("Support Vector Machine Metrics:")
print(f"Accuracy: {accuracy_svm:.4f}")
print(f"Precision: {precision_svm:.4f}")
print(f"Recall: {recall_svm:.4f}")
print(f"F1-score: {f1_svm:.4f}")
print("-" * 30)

# Evaluate Random Forest
y_pred_rf = rf_model.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
print("Random Forest Metrics:")
print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-score: {f1_rf:.4f}")
print("-" * 30)

# Evaluate Gradient Boosting
y_pred_gb = gb_model.predict(X_test)
accuracy_gb = accuracy_score(y_test, y_pred_gb)
precision_gb = precision_score(y_test, y_pred_gb)
recall_gb = recall_score(y_test, y_pred_gb)
f1_gb = f1_score(y_test, y_pred_gb)
print("Gradient Boosting Metrics:")
print(f"Accuracy: {accuracy_gb:.4f}")
print(f"Precision: {precision_gb:.4f}")
print(f"Recall: {recall_gb:.4f}")
print(f"F1-score: {f1_gb:.4f}")
print("-" * 30)

# Evaluate K-Nearest Neighbors
y_pred_knn = knn_model.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
print("K-Nearest Neighbors Metrics:")
print(f"Accuracy: {accuracy_knn:.4f}")
print(f"Precision: {precision_knn:.4f}")
print(f"Recall: {recall_knn:.4f}")
print(f"F1-score: {f1_knn:.4f}")
print("-" * 30)

"""**Reasoning**:
Summarize the performance metrics for each model and identify the best performing model based on the F1-score.


"""

# Summarize the performance of each model
print("Model Performance Summary:")
print("-" * 40)
print(f"Logistic Regression:")
print(f"  Accuracy: {accuracy_lr:.4f}")
print(f"  Precision: {precision_lr:.4f}")
print(f"  Recall: {recall_lr:.4f}")
print(f"  F1-score: {f1_lr:.4f}")
print("-" * 40)

print(f"Support Vector Machine:")
print(f"  Accuracy: {accuracy_svm:.4f}")
print(f"  Precision: {precision_svm:.4f}")
print(f"  Recall: {recall_svm:.4f}")
print(f"  F1-score: {f1_svm:.4f}")
print("-" * 40)

print(f"Random Forest:")
print(f"  Accuracy: {accuracy_rf:.4f}")
print(f"  Precision: {precision_rf:.4f}")
print(f"  Recall: {recall_rf:.4f}")
print(f"  F1-score: {f1_rf:.4f}")
print("-" * 40)

print(f"Gradient Boosting:")
print(f"  Accuracy: {accuracy_gb:.4f}")
print(f"  Precision: {precision_gb:.4f}")
print(f"  Recall: {recall_gb:.4f}")
print(f"  F1-score: {f1_gb:.4f}")
print("-" * 40)

print(f"K-Nearest Neighbors:")
print(f"  Accuracy: {accuracy_knn:.4f}")
print(f"  Precision: {precision_knn:.4f}")
print(f"  Recall: {recall_knn:.4f}")
print(f"  F1-score: {f1_knn:.4f}")
print("-" * 40)

# Identify the best performing model based on F1-score
# Note: Random Forest and Gradient Boosting have perfect scores, which might indicate overfitting.
# Considering a balance of metrics and potential overfitting, SVM shows strong performance without perfect scores.
# However, based strictly on the provided metrics, RF and GB are the "best".
# Let's explicitly state this based on the F1-score.

best_f1 = max(f1_lr, f1_svm, f1_rf, f1_gb, f1_knn)

if best_f1 == f1_rf:
    best_model_name = "Random Forest"
elif best_f1 == f1_gb:
    best_model_name = "Gradient Boosting"
elif best_f1 == f1_svm:
    best_model_name = "Support Vector Machine"
elif best_f1 == f1_knn:
    best_model_name = "K-Nearest Neighbors"
else:
    best_model_name = "Logistic Regression"

print(f"\nBased on F1-score, the best performing model is: {best_model_name}")

"""**Reasoning**:
Import the necessary libraries for hyperparameter tuning, define the parameter grid, instantiate GridSearchCV, and fit it to the training data. Then print the best parameters found.


"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Instantiate a Random Forest model
rf_model_tune = RandomForestClassifier(random_state=42)

# Instantiate GridSearchCV
grid_search_rf = GridSearchCV(rf_model_tune, param_grid_rf, cv=5, scoring='f1', n_jobs=-1) # Use n_jobs=-1 to utilize all available cores

# Fit GridSearchCV to the training data
grid_search_rf.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters found for Random Forest:")
print(grid_search_rf.best_params_)

"""**Reasoning**:
Instantiate a new Random Forest model with the best hyperparameters found, train it on the training data, make predictions on the test data, and evaluate its performance using accuracy, precision, recall, and F1-score to see if tuning improved the results.


"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Instantiate a new Random Forest model with the best hyperparameters
best_rf_model = RandomForestClassifier(n_estimators=grid_search_rf.best_params_['n_estimators'],
                                       max_depth=grid_search_rf.best_params_['max_depth'],
                                       min_samples_split=grid_search_rf.best_params_['min_samples_split'],
                                       min_samples_leaf=grid_search_rf.best_params_['min_samples_leaf'],
                                       random_state=42)

# Train the best model on the entire training dataset
best_rf_model.fit(X_train, y_train)

# Make predictions on the testing dataset
y_pred_rf_tuned = best_rf_model.predict(X_test)

# Calculate evaluation metrics for the tuned Random Forest model
accuracy_rf_tuned = accuracy_score(y_test, y_pred_rf_tuned)
precision_rf_tuned = precision_score(y_test, y_pred_rf_tuned)
recall_rf_tuned = recall_score(y_test, y_pred_rf_tuned)
f1_rf_tuned = f1_score(y_test, y_pred_rf_tuned)

# Print the evaluation metrics
print("Metrics for the initial Random Forest model:")
print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-score: {f1_rf:.4f}")
print("\nMetrics for the tuned Random Forest model:")
print(f"Accuracy: {accuracy_rf_tuned:.4f}")
print(f"Precision: {precision_rf_tuned:.4f}")
print(f"Recall: {recall_rf_tuned:.4f}")
print(f"F1-score: {f1_rf_tuned:.4f}")

"""**Reasoning**:
Calculate the correlation matrix for the numerical columns and then visualize it as a heatmap with annotations to show the correlation values.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Select only numerical columns for correlation analysis
numerical_data = data.select_dtypes(include=['float64', 'int64'])

# Calculate the correlation matrix
correlation_matrix = numerical_data.corr()

# Create a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Matriks Korelasi Variabel Numerik')
plt.show()

# Display correlations with 'CKD_Status'
print("\nKorelasi dengan CKD_Status:")
print(correlation_matrix['CKD_Status'].sort_values(ascending=False))

"""**Reasoning**:
List at least three different classification algorithms suitable for a binary classification problem and provide a brief justification for selecting each model.
"""

print("Suitable Classification Models for Binary Classification:")

print("\n1. Logistic Regression:")
print("   - Justification: A simple yet powerful linear model that estimates the probability of a binary outcome. It's computationally efficient and provides interpretable coefficients, making it a good baseline model.")

print("\n2. Support Vector Machine (SVM):")
print("   - Justification: Effective in high-dimensional spaces and when there is a clear margin of separation between classes. SVMs can use different kernel functions to handle non-linear relationships.")

print("\n3. Random Forest:")
print("   - Justification: An ensemble method that builds multiple decision trees and merges their predictions. It's robust to overfitting, can handle non-linear data, and provides feature importance scores.")

print("\n4. Gradient Boosting (e.g., LightGBM, XGBoost, CatBoost):")
print("   - Justification: Another powerful ensemble technique that builds trees sequentially, with each new tree correcting errors of the previous ones. Often achieves high accuracy and can capture complex patterns.")

print("\n5. K-Nearest Neighbors (KNN):")
print("   - Justification: A non-parametric lazy learning algorithm that classifies data points based on the majority class of their nearest neighbors. Simple to understand and implement, and effective for datasets with well-separated classes.")

"""**Reasoning**:
Import the selected classification models and train each one on the training data.
"""

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

# Instantiate the selected classification models
# Logistic Regression is already imported and instantiated as 'model' in a previous cell
svm_model = SVC(random_state=42)
rf_model = RandomForestClassifier(random_state=42)
gb_model = GradientBoostingClassifier(random_state=42)
knn_model = KNeighborsClassifier() # KNN does not have a random_state

# Train each model on the training data
print("Training Logistic Regression model...")
model.fit(X_train, y_train)
print("Logistic Regression model trained.")

print("Training Support Vector Machine model...")
svm_model.fit(X_train, y_train)
print("Support Vector Machine model trained.")

print("Training Random Forest model...")
rf_model.fit(X_train, y_train)
print("Random Forest model trained.")

print("Training Gradient Boosting model...")
gb_model.fit(X_train, y_train)
print("Gradient Boosting model trained.")

print("Training K-Nearest Neighbors model...")
knn_model.fit(X_train, y_train)
print("K-Nearest Neighbors model trained.")

"""**Reasoning**:
Evaluate the performance of each trained model on the testing data using accuracy, precision, recall, and F1-score and print the results.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Evaluate Logistic Regression
y_pred_lr = model.predict(X_test)
accuracy_lr = accuracy_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)
print("Logistic Regression Metrics:")
print(f"Accuracy: {accuracy_lr:.4f}")
print(f"Precision: {precision_lr:.4f}")
print(f"Recall: {recall_lr:.4f}")
print(f"F1-score: {f1_lr:.4f}")
print("-" * 30)

# Evaluate Support Vector Machine
y_pred_svm = svm_model.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm)
recall_svm = recall_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm)
print("Support Vector Machine Metrics:")
print(f"Accuracy: {accuracy_svm:.4f}")
print(f"Precision: {precision_svm:.4f}")
print(f"Recall: {recall_svm:.4f}")
print(f"F1-score: {f1_svm:.4f}")
print("-" * 30)

# Evaluate Random Forest
y_pred_rf = rf_model.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
print("Random Forest Metrics:")
print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-score: {f1_rf:.4f}")
print("-" * 30)

# Evaluate Gradient Boosting
y_pred_gb = gb_model.predict(X_test)
accuracy_gb = accuracy_score(y_test, y_pred_gb)
precision_gb = precision_score(y_test, y_pred_gb)
recall_gb = recall_score(y_test, y_pred_gb)
f1_gb = f1_score(y_test, y_pred_gb)
print("Gradient Boosting Metrics:")
print(f"Accuracy: {accuracy_gb:.4f}")
print(f"Precision: {precision_gb:.4f}")
print(f"Recall: {recall_gb:.4f}")
print(f"F1-score: {f1_gb:.4f}")
print("-" * 30)

# Evaluate K-Nearest Neighbors
y_pred_knn = knn_model.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
print("K-Nearest Neighbors Metrics:")
print(f"Accuracy: {accuracy_knn:.4f}")
print(f"Precision: {precision_knn:.4f}")
print(f"Recall: {recall_knn:.4f}")
print(f"F1-score: {f1_knn:.4f}")
print("-" * 30)

"""**Reasoning**:
Summarize the performance metrics for each model and identify the best performing model based on the F1-score.
"""

# Summarize the performance of each model
print("Model Performance Summary:")
print("-" * 40)
print(f"Logistic Regression:")
print(f"  Accuracy: {accuracy_lr:.4f}")
print(f"  Precision: {precision_lr:.4f}")
print(f"  Recall: {recall_lr:.4f}")
print(f"  F1-score: {f1_lr:.4f}")
print("-" * 40)

print(f"Support Vector Machine:")
print(f"  Accuracy: {accuracy_svm:.4f}")
print(f"  Precision: {precision_svm:.4f}")
print(f"  Recall: {recall_svm:.4f}")
print(f"  F1-score: {f1_svm:.4f}")
print("-" * 40)

print(f"Random Forest:")
print(f"  Accuracy: {accuracy_rf:.4f}")
print(f"  Precision: {precision_rf:.4f}")
print(f"  Recall: {recall_rf:.4f}")
print(f"  F1-score: {f1_rf:.4f}")
print("-" * 40)

print(f"Gradient Boosting:")
print(f"  Accuracy: {accuracy_gb:.4f}")
print(f"  Precision: {precision_gb:.4f}")
print(f"  Recall: {recall_gb:.4f}")
print(f"  F1-score: {f1_gb:.4f}")
print("-" * 40)

print(f"K-Nearest Neighbors:")
print(f"  Accuracy: {accuracy_knn:.4f}")
print(f"  Precision: {precision_knn:.4f}")
print(f"  Recall: {recall_knn:.4f}")
print(f"  F1-score: {f1_knn:.4f}")
print("-" * 40)

# Identify the best performing model based on F1-score
# Note: Random Forest and Gradient Boosting have perfect scores, which might indicate overfitting.
# Considering a balance of metrics and potential overfitting, SVM shows strong performance without perfect scores.
# However, based strictly on the provided metrics, RF and GB are the "best".
# Let's explicitly state this based on the F1-score.

best_f1 = max(f1_lr, f1_svm, f1_rf, f1_gb, f1_knn)

if best_f1 == f1_rf:
    best_model_name = "Random Forest"
elif best_f1 == f1_gb:
    best_model_name = "Gradient Boosting"
elif best_f1 == f1_svm:
    best_model_name = "Support Vector Machine"
elif best_f1 == f1_knn:
    best_model_name = "K-Nearest Neighbors"
else:
    best_model_name = "Logistic Regression"

print(f"\nBased on F1-score, the best performing model is: {best_model_name}")

"""**Reasoning**:
Import the necessary libraries for hyperparameter tuning, define the parameter grid, instantiate GridSearchCV, and fit it to the training data. Then print the best parameters found.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Instantiate a Random Forest model
rf_model_tune = RandomForestClassifier(random_state=42)

# Instantiate GridSearchCV
grid_search_rf = GridSearchCV(rf_model_tune, param_grid_rf, cv=5, scoring='f1', n_jobs=-1) # Use n_jobs=-1 to utilize all available cores

# Fit GridSearchCV to the training data
grid_search_rf.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters found for Random Forest:")
print(grid_search_rf.best_params_)

"""**Reasoning**:
Instantiate a new Random Forest model with the best hyperparameters found, train it on the training data, make predictions on the test data, and evaluate its performance using accuracy, precision, recall, and F1-score to see if tuning improved the results.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Instantiate a new Random Forest model with the best hyperparameters
best_rf_model = RandomForestClassifier(n_estimators=grid_search_rf.best_params_['n_estimators'],
                                       max_depth=grid_search_rf.best_params_['max_depth'],
                                       min_samples_split=grid_search_rf.best_params_['min_samples_split'],
                                       min_samples_leaf=grid_search_rf.best_params_['min_samples_leaf'],
                                       random_state=42)

# Train the best model on the entire training dataset
best_rf_model.fit(X_train, y_train)

# Make predictions on the testing dataset
y_pred_rf_tuned = best_rf_model.predict(X_test)

# Calculate evaluation metrics for the tuned Random Forest model
accuracy_rf_tuned = accuracy_score(y_test, y_pred_rf_tuned)
precision_rf_tuned = precision_score(y_test, y_pred_rf_tuned)
recall_rf_tuned = recall_score(y_test, y_pred_rf_tuned)
f1_rf_tuned = f1_score(y_test, y_pred_rf_tuned)

# Print the evaluation metrics
print("Metrics for the initial Random Forest model:")
print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-score: {f1_rf:.4f}")
print("\nMetrics for the tuned Random Forest model:")
print(f"Accuracy: {accuracy_rf_tuned:.4f}")
print(f"Precision: {precision_rf_tuned:.4f}")
print(f"Recall: {recall_rf_tuned:.4f}")
print(f"F1-score: {f1_rf_tuned:.4f}")

"""**Reasoning**:
Import KFold and instantiate it with the specified number of splits, shuffle, and random state, then print the number of splits.


"""

from sklearn.model_selection import KFold

# Instantiate KFold with 5 splits, shuffling, and a random state
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Print the number of splits
print(f"Number of splits for K-Fold Cross-Validation: {kf.get_n_splits()}")

"""**Reasoning**:
Import cross_val_score, select the Random Forest model, perform 5-fold cross-validation on the training data using cross_val_score with the defined kf splits and F1 scoring, store the scores, and print the results.


"""

from sklearn.model_selection import cross_val_score

# Select the Random Forest model for cross-validation
# rf_model was instantiated in a previous cell (cell_id: 1395ea10, a0501d1e)

# Perform 5-fold cross-validation on the training data
cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='f1')

# Print the cross-validation scores
print("Cross-validation F1 scores for Random Forest:")
print(cv_scores_rf)
print(f"Mean F1-score: {cv_scores_rf.mean():.4f}")
print(f"Standard deviation of F1-scores: {cv_scores_rf.std():.4f}")

"""**Reasoning**:
Evaluate the performance of each trained model on the testing data using accuracy, precision, recall, and F1-score and print the results.


"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer

# Define the scoring metrics to evaluate
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score)
}

# Evaluate Random Forest using cross-validation for multiple metrics
cv_results_rf = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='f1')
cv_results_rf_accuracy = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='accuracy')
cv_results_rf_precision = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='precision')
cv_results_rf_recall = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='recall')


# Calculate and print the mean and standard deviation for each metric
print("Random Forest Cross-Validation Results:")
print(f"F1-score: Mean = {cv_results_rf.mean():.4f}, Std = {cv_results_rf.std():.4f}")
print(f"Accuracy: Mean = {cv_results_rf_accuracy.mean():.4f}, Std = {cv_results_rf_accuracy.std():.4f}")
print(f"Precision: Mean = {cv_results_rf_precision.mean():.4f}, Std = {cv_results_rf_precision.std():.4f}")
print(f"Recall: Mean = {cv_results_rf_recall.mean():.4f}, Std = {cv_results_rf_recall.std():.4f}")

"""**Reasoning**:
Random Forest does not have direct L1/L2 regularization parameters like Logistic Regression. However, hyperparameter tuning (which has already been performed and resulted in perfect scores, potentially indicating overfitting) and ensemble techniques are methods to manage overfitting in Random Forests. Since the previous tuning resulted in perfect scores on the test set, which is unusual and could indicate data leakage or a very simple dataset, I will re-evaluate the tuned model on the test set to confirm the metrics. Then I will finish the task as Random Forest does not support L1/L2 regularization directly, and the hyperparameter tuning step already addressed potential overfitting through tree structure controls.


"""

# Re-evaluate the best tuned Random Forest model on the test set
y_pred_rf_tuned = best_rf_model.predict(X_test)

accuracy_rf_tuned = accuracy_score(y_test, y_pred_rf_tuned)
precision_rf_tuned = precision_score(y_test, y_pred_rf_tuned)
recall_rf_tuned = recall_score(y_test, y_pred_rf_tuned)
f1_rf_tuned = f1_score(y_test, y_pred_rf_tuned)

print("Metrics for the tuned Random Forest model on the test set:")
print(f"Accuracy: {accuracy_rf_tuned:.4f}")
print(f"Precision: {precision_rf_tuned:.4f}")
print(f"Recall: {recall_rf_tuned:.4f}")
print(f"F1-score: {f1_rf_tuned:.4f}")

print("\nRandom Forest does not directly support L1/L2 regularization.")
print("Overfitting in Random Forests is typically addressed through hyperparameter tuning (max_depth, min_samples_split, min_samples_leaf, etc.) which was already performed.")
print("Given the perfect scores, it's possible the dataset is highly separable, or there might be an issue, but based on the provided data and standard procedures, regularization as typically applied to linear models is not applicable here.")

"""**Reasoning**:
Define the parameter grid for the best performing model (Random Forest), instantiate GridSearchCV with the Random Forest model and the defined parameter grid, and fit it to the training data. Then print the best parameters found.


"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, KFold

# Instantiate KFold with 5 splits, shuffling, and a random state (already done, but re-instantiate to be safe in this cell)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Define the parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Instantiate a Random Forest model
rf_model_tune = RandomForestClassifier(random_state=42)

# Instantiate GridSearchCV with the Random Forest model, the defined parameter grid, and KFold cross-validation
grid_search_rf = GridSearchCV(rf_model_tune, param_grid_rf, cv=kf, scoring='f1', n_jobs=-1) # Use n_jobs=-1 for parallel processing

# Fit GridSearchCV to the training data
grid_search_rf.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters found for Random Forest using cross-validation:")
print(grid_search_rf.best_params_)

"""**Reasoning**:
Instantiate the final Random Forest model with the best hyperparameters, train it on the training data, make predictions on the test data, calculate evaluation metrics, and print the results to report the final performance.


"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Instantiate the final Random Forest model with the best hyperparameters
final_rf_model = RandomForestClassifier(n_estimators=grid_search_rf.best_params_['n_estimators'],
                                        max_depth=grid_search_rf.best_params_['max_depth'],
                                        min_samples_split=grid_search_rf.best_params_['min_samples_split'],
                                        min_samples_leaf=grid_search_rf.best_params_['min_samples_leaf'],
                                        random_state=42)

# Train the final model on the entire training dataset
final_rf_model.fit(X_train, y_train)

# Make predictions on the held-out testing dataset
y_pred_final = final_rf_model.predict(X_test)

# Calculate the final evaluation metrics
accuracy_final = accuracy_score(y_test, y_pred_final)
precision_final = precision_score(y_test, y_pred_final)
recall_final = recall_score(y_test, y_pred_final)
f1_final = f1_score(y_test, y_pred_final)

# Print the final performance metrics
print("Final Random Forest Model Performance on the Test Set:")
print(f"Accuracy: {accuracy_final:.4f}")
print(f"Precision: {precision_final:.4f}")
print(f"Recall: {recall_final:.4f}")
print(f"F1-score: {f1_final:.4f}")

"""## Summary:

### Data Analysis Key Findings

*   Cross-validation was successfully performed on the Random Forest model using 5 folds, resulting in a mean F1-score of 0.9995 with a standard deviation of 0.0011 on the training data.
*   The Random Forest model does not directly support L1 or L2 regularization; overfitting is typically addressed through hyperparameter tuning.
*   Hyperparameter tuning for the Random Forest model using GridSearchCV and 5-fold cross-validation identified the best parameters as: `'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100`.
*   The final Random Forest model, trained with the best hyperparameters, achieved perfect scores (1.0000) for Accuracy, Precision, Recall, and F1-score on the test set.

### Insights or Next Steps

*   The perfect performance metrics on both the cross-validation folds and the test set suggest that the dataset might be highly separable. Further investigation into the data characteristics and potential data leakage is recommended to confirm the reliability of these results.
*   Given the high performance, the next steps could involve deploying the model or exploring its interpretability to understand which features are most influential in predicting tumor malignancy, assuming the perfect scores are validated.

**Reasoning**:
Create visualizations as requested: histograms for numerical columns, a scatter plot for 'Creatinine_Level' vs 'BUN', and a box plot for 'GFR' by 'CKD_Status' to understand variable distributions and relationships.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Histograms for numerical columns
numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    plt.subplot(3, 3, i + 1)
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Step 2: Scatter plot for 'Creatinine_Level' and 'BUN'
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Creatinine_Level', y='BUN', data=data)
plt.title('Creatinine Level vs BUN')
plt.xlabel('Creatinine Level')
plt.ylabel('BUN')
plt.show()

# Step 3: Box plot for 'GFR' by 'CKD_Status'
plt.figure(figsize=(8, 6))
sns.boxplot(x='CKD_Status', y='GFR', data=data)
plt.title('GFR Distribution by CKD Status')
plt.xlabel('CKD Status (0: Non-CKD, 1: CKD)')
plt.ylabel('GFR')
plt.xticks([0, 1], ['Non-CKD', 'CKD'])
plt.show()

"""**Reasoning**:
Import KFold and instantiate it with the specified number of splits, shuffle, and random state, then print the number of splits.
"""